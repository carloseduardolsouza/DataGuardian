# üéØ DataGuardian - Arquitetura Definitiva

## üìã Vis√£o Geral

Sistema **self-hosted open-source** de gerenciamento de backups de bancos de dados e arquivos.

- **Single-user**: Sem necessidade de autentica√ß√£o complexa
- **Monolito**: API + Workers em um √∫nico processo Node.js
- **TypeScript**: Type-safety e melhor DX
- **Prisma ORM**: Migrations e type-safe queries
- **Docker**: Deploy simplificado via docker-compose

---

## üóÑÔ∏è Schema do Banco de Dados (PostgreSQL)

### **datasources**
Bancos de dados e sistemas de arquivos que ser√£o backupeados.

| Campo | Tipo | Descri√ß√£o |
|-------|------|-----------|
| `id` | UUID | Primary key |
| `name` | VARCHAR(255) | Nome amig√°vel (ex: "Banco Produ√ß√£o") |
| `type` | ENUM | `postgres`, `mysql`, `mongodb`, `sqlserver`, `sqlite`, `files` |
| `connection_config` | JSONB | Configura√ß√£o de conex√£o (estrutura varia por tipo) |
| `status` | ENUM | `healthy`, `warning`, `critical`, `unknown` |
| `last_health_check_at` | TIMESTAMP | √öltima verifica√ß√£o de sa√∫de |
| `enabled` | BOOLEAN | Se est√° ativo para backups |
| `tags` | TEXT[] | Array de tags (ex: ["produ√ß√£o", "cr√≠tico"]) |
| `created_at` | TIMESTAMP | Data de cria√ß√£o |
| `updated_at` | TIMESTAMP | √öltima atualiza√ß√£o |

**Estrutura de `connection_config` por tipo:**

```typescript
// Postgres, MySQL, SQL Server, MongoDB
{
  host: string;
  port: number;
  database: string;
  username: string;
  password: string;
  ssl_enabled?: boolean;
}

// SQLite
{
  file_path: string;
}

// Files
{
  source_path: string;
  include_patterns?: string[];  // ["*.jpg", "*.png"]
  exclude_patterns?: string[];  // ["*.log", "temp/*"]
}
```

---

### **storage_locations**
Locais onde os backups ser√£o salvos.

| Campo | Tipo | Descri√ß√£o |
|-------|------|-----------|
| `id` | UUID | Primary key |
| `name` | VARCHAR(255) | Nome amig√°vel (ex: "NAS Empresa") |
| `type` | ENUM | `local`, `s3`, `ssh`, `minio`, `backblaze` |
| `config` | JSONB | Configura√ß√£o espec√≠fica do storage |
| `is_default` | BOOLEAN | Se √© o storage padr√£o |
| `available_space_gb` | DECIMAL | Espa√ßo dispon√≠vel (atualizado periodicamente) |
| `status` | ENUM | `healthy`, `full`, `unreachable` |
| `created_at` | TIMESTAMP | Data de cria√ß√£o |
| `updated_at` | TIMESTAMP | √öltima atualiza√ß√£o |

**Estrutura de `config` por tipo:**

```typescript
// Local
{
  path: string;              // "/var/backups"
  max_size_gb?: number;      // Alerta quando atingir limite
}

// SSH/SFTP
{
  host: string;
  port: number;              // Padr√£o 22
  username: string;
  password?: string;
  private_key?: string;      // Conte√∫do da chave SSH
  remote_path: string;       // "/mnt/storage/backups"
}

// S3 (AWS/Wasabi)
{
  endpoint?: string;         // null para AWS padr√£o
  bucket: string;
  region: string;
  access_key_id: string;
  secret_access_key: string;
  storage_class?: string;    // "STANDARD_IA", "GLACIER"
}

// MinIO
{
  endpoint: string;          // "http://minio.local:9000"
  bucket: string;
  access_key: string;
  secret_key: string;
  use_ssl: boolean;
}

// Backblaze B2
{
  bucket_id: string;
  bucket_name: string;
  application_key_id: string;
  application_key: string;
}
```

---

### **backup_jobs**
Pol√≠ticas de backup configuradas pelo usu√°rio.

| Campo | Tipo | Descri√ß√£o |
|-------|------|-----------|
| `id` | UUID | Primary key |
| `name` | VARCHAR(255) | Nome do job (ex: "Backup Di√°rio Produ√ß√£o") |
| `datasource_id` | UUID | FK ‚Üí datasources.id |
| `storage_location_id` | UUID | FK ‚Üí storage_locations.id |
| `schedule_cron` | VARCHAR(100) | Express√£o cron (ex: "0 3 * * *") |
| `schedule_timezone` | VARCHAR(50) | Timezone (ex: "America/Sao_Paulo") |
| `enabled` | BOOLEAN | Se o job est√° ativo |
| `retention_policy` | JSONB | Regras de reten√ß√£o |
| `backup_options` | JSONB | Op√ß√µes espec√≠ficas do backup |
| `last_execution_at` | TIMESTAMP | √öltima execu√ß√£o |
| `next_execution_at` | TIMESTAMP | Pr√≥xima execu√ß√£o calculada |
| `created_at` | TIMESTAMP | Data de cria√ß√£o |
| `updated_at` | TIMESTAMP | √öltima atualiza√ß√£o |

**Estrutura de `retention_policy`:**

```typescript
{
  keep_daily: number;        // Manter backups di√°rios por X dias
  keep_weekly: number;       // Manter backups semanais por X semanas
  keep_monthly: number;      // Manter backups mensais por X meses
  auto_delete: boolean;      // Deletar automaticamente backups antigos
}
```

**Estrutura de `backup_options`:**

```typescript
{
  compression: "gzip" | "zstd" | "lz4" | "none";
  compression_level?: number;       // 1-9 (gzip/zstd)
  parallel_jobs?: number;           // Para pg_dump -j
  exclude_tables?: string[];        // ["logs_*", "temp_*"]
  include_tables?: string[];        // [] = todas
  max_file_size_mb?: number;        // Dividir em chunks se maior
}
```

---

### **backup_executions**
Hist√≥rico de execu√ß√µes de backup.

| Campo | Tipo | Descri√ß√£o |
|-------|------|-----------|
| `id` | UUID | Primary key |
| `job_id` | UUID | FK ‚Üí backup_jobs.id |
| `datasource_id` | UUID | FK ‚Üí datasources.id |
| `storage_location_id` | UUID | FK ‚Üí storage_locations.id |
| `status` | ENUM | `queued`, `running`, `completed`, `failed`, `cancelled` |
| `started_at` | TIMESTAMP | In√≠cio da execu√ß√£o |
| `finished_at` | TIMESTAMP | Fim da execu√ß√£o |
| `duration_seconds` | INTEGER | Dura√ß√£o total |
| `size_bytes` | BIGINT | Tamanho original dos dados |
| `compressed_size_bytes` | BIGINT | Tamanho ap√≥s compress√£o |
| `backup_path` | TEXT | Caminho completo no storage |
| `backup_type` | ENUM | `full`, `incremental`, `differential` |
| `files_count` | INTEGER | Quantidade de arquivos (para backup de files) |
| `error_message` | TEXT | Mensagem de erro (se falhou) |
| `error_stack` | TEXT | Stack trace completo |
| `metadata` | JSONB | Informa√ß√µes adicionais |
| `created_at` | TIMESTAMP | Data de cria√ß√£o |

**Estrutura de `metadata`:**

```typescript
{
  database_version?: string;       // "PostgreSQL 16.1"
  tables_backed_up?: number;       // Quantidade de tabelas
  rows_approximate?: number;       // Estimativa de linhas
  compression_ratio?: number;      // 0.35 = 65% de compress√£o
  checksum?: string;               // SHA256 do arquivo final
  engine_output?: string;          // Sa√≠da do pg_dump/mysqldump
}
```

---

### **backup_chunks**
Para backups grandes divididos em m√∫ltiplos arquivos.

| Campo | Tipo | Descri√ß√£o |
|-------|------|-----------|
| `id` | UUID | Primary key |
| `execution_id` | UUID | FK ‚Üí backup_executions.id |
| `chunk_number` | INTEGER | N√∫mero sequencial do chunk (1, 2, 3...) |
| `file_path` | TEXT | Caminho do chunk no storage |
| `size_bytes` | BIGINT | Tamanho do chunk |
| `checksum` | VARCHAR(64) | SHA256 do chunk |
| `created_at` | TIMESTAMP | Data de cria√ß√£o |

**Unique constraint**: `(execution_id, chunk_number)`

---

### **health_checks**
Hist√≥rico de verifica√ß√µes de sa√∫de das datasources.

| Campo | Tipo | Descri√ß√£o |
|-------|------|-----------|
| `id` | UUID | Primary key |
| `datasource_id` | UUID | FK ‚Üí datasources.id |
| `checked_at` | TIMESTAMP | Momento da verifica√ß√£o |
| `status` | ENUM | `ok`, `timeout`, `auth_failed`, `unreachable`, `error` |
| `latency_ms` | INTEGER | Tempo de resposta em ms |
| `error_message` | TEXT | Mensagem de erro (se houver) |
| `metadata` | JSONB | Informa√ß√µes adicionais |

**Estrutura de `metadata`:**

```typescript
{
  database_version?: string;
  server_uptime?: string;
  active_connections?: number;
  disk_usage_percent?: number;
}
```

**Index**: `(datasource_id, checked_at DESC)`

---

### **notifications**
Sistema de alertas e eventos do sistema.

| Campo | Tipo | Descri√ß√£o |
|-------|------|-----------|
| `id` | UUID | Primary key |
| `type` | ENUM | `backup_failed`, `backup_success`, `connection_lost`, `storage_full`, `storage_unreachable`, `health_degraded` |
| `severity` | ENUM | `info`, `warning`, `critical` |
| `entity_type` | ENUM | `datasource`, `backup_job`, `storage_location`, `system` |
| `entity_id` | UUID | ID da entidade relacionada |
| `title` | VARCHAR(255) | T√≠tulo curto do alerta |
| `message` | TEXT | Descri√ß√£o detalhada |
| `metadata` | JSONB | Dados adicionais contextuais |
| `read_at` | TIMESTAMP | Quando foi marcado como lido |
| `created_at` | TIMESTAMP | Data de cria√ß√£o |

**Index**: `(created_at DESC, read_at)`

---

### **system_settings**
Configura√ß√µes globais do sistema.

| Campo | Tipo | Descri√ß√£o |
|-------|------|-----------|
| `key` | VARCHAR(100) | Primary key - nome da configura√ß√£o |
| `value` | JSONB | Valor da configura√ß√£o |
| `description` | TEXT | Descri√ß√£o da configura√ß√£o |
| `updated_at` | TIMESTAMP | √öltima atualiza√ß√£o |

**Exemplos de `key`:**
- `notifications.email_enabled`
- `notifications.email_smtp_config`
- `notifications.webhook_url`
- `system.max_concurrent_backups`
- `system.temp_directory`

---

## üìä Relacionamentos

```
datasources (1) ‚îÄ‚îÄ< (N) backup_jobs
datasources (1) ‚îÄ‚îÄ< (N) health_checks
datasources (1) ‚îÄ‚îÄ< (N) backup_executions

storage_locations (1) ‚îÄ‚îÄ< (N) backup_jobs
storage_locations (1) ‚îÄ‚îÄ< (N) backup_executions

backup_jobs (1) ‚îÄ‚îÄ< (N) backup_executions

backup_executions (1) ‚îÄ‚îÄ< (N) backup_chunks
backup_executions (1) ‚îÄ‚îÄ< (N) notifications (via entity_id)

datasources (1) ‚îÄ‚îÄ< (N) notifications (via entity_id)
backup_jobs (1) ‚îÄ‚îÄ< (N) notifications (via entity_id)
storage_locations (1) ‚îÄ‚îÄ< (N) notifications (via entity_id)
```

---

## üìÅ Estrutura de Diret√≥rios

```
backup-manager/
‚îÇ
‚îú‚îÄ‚îÄ prisma/
‚îÇ   ‚îú‚îÄ‚îÄ schema.prisma              # Schema do Prisma
‚îÇ   ‚îú‚îÄ‚îÄ migrations/                # Migrations autom√°ticas
‚îÇ   ‚îî‚îÄ‚îÄ seed.ts                    # Dados iniciais (opcional)
‚îÇ
‚îú‚îÄ‚îÄ interface/                     # interface em react
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ api/                       # Express REST API
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datasources.ts     # CRUD datasources
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ storage-locations.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ backup-jobs.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ executions.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ health.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ notifications.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ system.ts
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ middlewares/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ error-handler.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validation.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.ts
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ server.ts              # Express app
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ core/                      # L√≥gica de neg√≥cio
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ backup/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ engines/           # Implementa√ß√µes por tipo de DB
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base-engine.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgres-engine.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mysql-engine.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mongodb-engine.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sqlserver-engine.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sqlite-engine.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ files-engine.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ executor.ts        # Orquestrador de backup
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ compressor.ts      # Gzip, zstd, lz4
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ storage/               # Abstra√ß√£o de storage
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adapters/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base-adapter.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ local-adapter.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ s3-adapter.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ssh-adapter.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ minio-adapter.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ backblaze-adapter.ts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ storage-factory.ts # Factory pattern
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scheduler/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cron-parser.ts     # Parse cron expressions
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ job-scheduler.ts   # Calcula pr√≥ximas execu√ß√µes
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retention/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cleanup-manager.ts # GFS retention policy
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ health/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ health-checker.ts  # Testa conex√µes
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ workers/                   # Background jobs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ backup-worker.ts       # Processa backups
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ health-worker.ts       # Health checks peri√≥dicos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scheduler-worker.ts    # Agenda backups
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cleanup-worker.ts      # Deleta backups antigos
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ queue/                     # BullMQ setup
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ queues.ts              # Defini√ß√£o das filas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ redis-client.ts        # Conex√£o Redis
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.ts              # Pino logger
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.ts              # Configura√ß√µes (env vars)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ notifications.ts       # Email/Webhook sender
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ types/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datasource.types.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ storage.types.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ backup.types.ts
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ index.ts                   # Entry point (inicia API + Workers)
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ default.json               # Configs padr√£o
‚îÇ   ‚îî‚îÄ‚îÄ production.json            # Configs de produ√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ development.json           # Configs de desenvolvimento
‚îÇ
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                 # Build da aplica√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml         # Stack completa
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ engines/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ retention/
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ integration/
‚îÇ       ‚îú‚îÄ‚îÄ api/
‚îÇ       ‚îî‚îÄ‚îÄ workers/
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ API.md                     # Documenta√ß√£o da API REST
‚îÇ   ‚îú‚îÄ‚îÄ STORAGE.md                 # Guia de storages suportados
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT.md              # Deploy com Docker
‚îÇ   ‚îî‚îÄ‚îÄ DEVELOPMENT.md             # Setup de dev
‚îÇ
‚îú‚îÄ‚îÄ .env.example                   # Template de vari√°veis
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ tsconfig.json
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ LICENSE
```

---

## üê≥ Docker Compose

```yaml
version: '3.8'

services:
  # Banco de metadados
  postgres:
    image: postgres:16-alpine
    container_name: backup-manager-db
    restart: unless-stopped
    environment:
      POSTGRES_DB: backup_manager
      POSTGRES_USER: backup
      POSTGRES_PASSWORD: ${DB_PASSWORD:-backup123}
    volumes:
      - postgres-data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U backup"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis para filas
  redis:
    image: redis:7-alpine
    container_name: backup-manager-redis
    restart: unless-stopped
    volumes:
      - redis-data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  # Aplica√ß√£o (API + Workers)
  app:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: backup-manager-app
    restart: unless-stopped
    ports:
      - "3000:3000"
    volumes:
      # Volume para backups locais
      - backup-storage:/var/backups
      # Volume para configs persistentes
      - ./config:/app/config:ro
    environment:
      NODE_ENV: production
      DATABASE_URL: postgresql://backup:${DB_PASSWORD:-backup123}@postgres:5432/backup_manager
      REDIS_URL: redis://redis:6379
      PORT: 3000
      LOG_LEVEL: info
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  postgres-data:
    driver: local
  redis-data:
    driver: local
  backup-storage:
    driver: local
```

---

## üîÑ Fluxo de Execu√ß√£o

### **1. Inicializa√ß√£o do Sistema**

```
docker-compose up -d
    ‚Üì
Container "app" inicia
    ‚Üì
src/index.ts executa:
    1. Conecta no PostgreSQL (Prisma)
    2. Roda migrations pendentes
    3. Conecta no Redis (BullMQ)
    4. Inicia Express API (porta 3000)
    5. Inicia Workers em background:
       - SchedulerWorker (verifica jobs a cada 1 min)
       - HealthWorker (health checks a cada 5 min)
       - BackupWorker (processa fila de backups)
       - CleanupWorker (roda 1x por dia √†s 4h)
    ‚Üì
Sistema pronto! üöÄ
```

---

### **2. Usu√°rio Configura Datasource**

```
Interface Web ‚Üí POST /api/datasources
    ‚Üì
Body: {
  name: "Banco Produ√ß√£o",
  type: "postgres",
  connection_config: {
    host: "db.empresa.local",
    port: 5432,
    database: "app_prod",
    username: "backup_user",
    password: "senhasegura"
  },
  tags: ["produ√ß√£o", "cr√≠tico"]
}
    ‚Üì
API valida dados
    ‚Üì
Testa conex√£o (SELECT 1)
    ‚úÖ Sucesso ‚Üí Salva no banco
    ‚ùå Falha ‚Üí Retorna erro 400
    ‚Üì
Agenda primeiro health check (em 5 min)
    ‚Üì
Retorna: { id: "uuid", status: "healthy", ... }
```

---

### **3. Usu√°rio Configura Storage Location**

```
Interface Web ‚Üí POST /api/storage-locations
    ‚Üì
Body: {
  name: "NAS Empresa",
  type: "ssh",
  config: {
    host: "nas.empresa.local",
    port: 22,
    username: "backup",
    private_key: "-----BEGIN RSA PRIVATE KEY-----\n...",
    remote_path: "/mnt/storage/backups"
  }
}
    ‚Üì
API valida dados
    ‚Üì
Testa conex√£o SSH
    ‚úÖ Sucesso ‚Üí Verifica espa√ßo dispon√≠vel
    ‚ùå Falha ‚Üí Retorna erro 400
    ‚Üì
Salva no banco
    ‚Üì
Retorna: { id: "uuid", available_space_gb: 450, ... }
```

---

### **4. Usu√°rio Cria Backup Job**

```
Interface Web ‚Üí POST /api/backup-jobs
    ‚Üì
Body: {
  name: "Backup Di√°rio Produ√ß√£o",
  datasource_id: "uuid-datasource",
  storage_location_id: "uuid-storage",
  schedule_cron: "0 3 * * *",
  schedule_timezone: "America/Sao_Paulo",
  retention_policy: {
    keep_daily: 7,
    keep_weekly: 4,
    keep_monthly: 12,
    auto_delete: true
  },
  backup_options: {
    compression: "gzip",
    compression_level: 6,
    parallel_jobs: 4,
    exclude_tables: ["logs_*", "temp_*"]
  }
}
    ‚Üì
API valida cron expression
    ‚Üì
Calcula next_execution_at (pr√≥ximo 3h da manh√£)
    ‚Üì
Salva no banco
    ‚Üì
SchedulerWorker detecta na pr√≥xima verifica√ß√£o
    ‚Üì
Retorna: { id: "uuid", next_execution_at: "2025-02-13T06:00:00Z", ... }
```

---

### **5. Execu√ß√£o do Backup (Agendado)**

```
SchedulerWorker (roda a cada 1 min)
    ‚Üì
SELECT jobs WHERE enabled = true
    ‚Üì
Para cada job:
    Agora >= next_execution_at?
    SIM ‚Üí
        1. Cria registro em backup_executions (status: queued)
        2. Adiciona job na fila Redis (backup-queue)
        3. Atualiza next_execution_at do job
    ‚Üì
BackupWorker (consumer da fila)
    ‚Üì
Pega job da fila
    ‚Üì
1. Atualiza execution (status: running, started_at)
2. Busca datasource e storage_location
3. Seleciona engine correto (PostgresEngine)
4. Cria stream de backup:
   
   PostgresEngine.backup()
       ‚Üì
   pg_dump --host=... --format=custom | gzip
       ‚Üì
   Stream de dados
       ‚Üì
   Compressor (gzip level 6)
       ‚Üì
   Chunker (se > max_file_size_mb)
       ‚Üì
   StorageAdapter.upload()
       ‚Üì
   SSH: rsync/sftp para NAS
   
5. Calcula checksum (SHA256)
6. Atualiza execution:
   - status: completed
   - finished_at
   - size_bytes, compressed_size_bytes
   - backup_path
   - metadata (compression_ratio, etc)
    ‚Üì
Sucesso! ‚úÖ
```

---

### **6. Health Check Cont√≠nuo**

```
HealthWorker (a cada 5 min)
    ‚Üì
SELECT datasources WHERE enabled = true
    ‚Üì
Para cada datasource:
    1. Conecta no banco
    2. Executa: SELECT 1
    3. Mede lat√™ncia
    4. Busca metadados (version, uptime)
    5. Salva em health_checks
    6. Atualiza datasource.status
    
    Se falhou 3x consecutivas:
        - Cria notification (type: connection_lost, severity: critical)
        - Envia alerta (email/webhook se configurado)
    
    Se voltou ap√≥s falha:
        - Cria notification (type: connection_restored, severity: info)
```

---

### **7. Cleanup de Backups Antigos**

```
CleanupWorker (1x por dia √†s 4h)
    ‚Üì
SELECT jobs WHERE retention_policy.auto_delete = true
    ‚Üì
Para cada job:
    1. Lista executions desse job (ORDER BY created_at DESC)
    2. Aplica regras GFS:
       
       √öltimos 7 dias ‚Üí mant√©m todos (daily)
       √öltimos 28 dias ‚Üí mant√©m apenas domingos (weekly)
       √öltimo ano ‚Üí mant√©m apenas dia 1 (monthly)
       
    3. Marca executions para deletar
    4. Para cada execution:
       - StorageAdapter.delete(backup_path)
       - DELETE FROM backup_chunks
       - DELETE FROM backup_executions
    5. Loga estat√≠sticas:
       - X backups deletados
       - Y GB liberados
    ‚Üì
Cria notification (type: cleanup_completed, severity: info)
```

---

## üõ†Ô∏è Tecnologias e Bibliotecas

### **Core**
- **Node.js** 20+
- **TypeScript** 5.x
- **Prisma ORM** 5.x
- **Express** 4.x
- **BullMQ** 5.x (filas com Redis)
- **Redis** 7.x

### **Backup Engines**
- `pg` - PostgreSQL client
- `mysql2` - MySQL/MariaDB client
- `mongodb` - MongoDB driver
- `mssql` - SQL Server client
- `better-sqlite3` - SQLite client

### **Storage Adapters**
- `@aws-sdk/client-s3` - AWS S3
- `ssh2` - SSH/SFTP
- `fs-extra` - Local filesystem
- `tar-stream` - Cria√ß√£o de tarballs

### **Compress√£o**
- `zlib` (nativo) - Gzip
- `@mongodb-js/zstd` - Zstandard
- `lz4` - LZ4

### **Utilit√°rios**
- `pino` - Logging estruturado
- `cron-parser` - Parse de cron expressions
- `zod` - Valida√ß√£o de schemas
- `date-fns-tz` - Manipula√ß√£o de datas com timezone

---

## üéØ Pr√≥ximos Passos

Com essa arquitetura definida, podemos come√ßar a implementa√ß√£o na seguinte ordem:

1. **Setup inicial**
   - Inicializar projeto TypeScript
   - Configurar Prisma + migrations
   - Setup Docker Compose

2. **Core b√°sico**
   - Interface base dos Storage Adapters
   - Interface base dos Backup Engines
   - LocalStorage + PostgresEngine (MVP)

3. **API REST**
   - CRUD de datasources
   - CRUD de storage locations
   - CRUD de backup jobs

4. **Workers**
   - BackupWorker (processa backups)
   - SchedulerWorker (agenda jobs)

5. **Expans√£o**
   - Mais engines (MySQL, MongoDB, Files)
   - Mais storages (S3, SSH, MinIO)
   - HealthWorker + CleanupWorker

---

**Pronto para come√ßar?** üöÄ
